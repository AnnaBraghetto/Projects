{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yNJ0bro4EbD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import gensim\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from math import ceil\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmbTBeQi9F9b"
   },
   "source": [
    "# Parameters of the Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1808,
     "status": "ok",
     "timestamp": 1581882955554,
     "user": {
      "displayName": "Marino Braghetto",
      "photoUrl": "",
      "userId": "02551736165510158038"
     },
     "user_tz": -60
    },
    "id": "S3VUVE6hZWJ0",
    "outputId": "f67a4f0a-c0ca-4bd6-d10a-3638a8addcc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I use cuda: True\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# GPU\n",
    "useCuda = False\n",
    "if torch.cuda.is_available():\n",
    "  useCuda = True\n",
    "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "print('I use cuda:',useCuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBFY7-Fl9FT8"
   },
   "outputs": [],
   "source": [
    "Colab = True\n",
    "\n",
    "### Boolean for processes\n",
    "boolW2V = False #Word to vec\n",
    "boolCV = False # Cross validation\n",
    "boolFT = False # Final training\n",
    "\n",
    "### Book\n",
    "books = ['Chance.txt','TheMirroroftheSea.txt','HeartofDarkness.txt','LordJim.txt','TheShadow-Line.txt']\n",
    "\n",
    "### Parameters\n",
    "\n",
    "# Dataset\n",
    "w2v_size = 100\n",
    "crop_len = 20\n",
    "batch_size = 512\n",
    "\n",
    "# Network\n",
    "dropout_prob = 0.3\n",
    "\n",
    "### Random seed\n",
    "np.random.seed(17)\n",
    "random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XOjn0Gx4Umh"
   },
   "source": [
    "# Colab instructions\n",
    "\n",
    "If I am using Colab, the path is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1790,
     "status": "ok",
     "timestamp": 1581882955555,
     "user": {
      "displayName": "Marino Braghetto",
      "photoUrl": "",
      "userId": "02551736165510158038"
     },
     "user_tz": -60
    },
    "id": "K4ADnlOm4MH8",
    "outputId": "0f904453-c96a-41bf-e0bd-1710c8fbdf15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "if Colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  path = '/content/drive/My Drive/'\n",
    "else:\n",
    "  path = ''      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kkfqtBVp4eLF"
   },
   "source": [
    "# Load of dataset \n",
    "\n",
    "The dataset is loaded and clean. Furthermore, just the most frequent words are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuvIr2aY40Vf"
   },
   "source": [
    "#### Clean dictionary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1YCICbk4zfB"
   },
   "outputs": [],
   "source": [
    "def BuildText(path,books):\n",
    "    \n",
    "    # Read files\n",
    "    text_list = [open(path+b, 'r').read() for b in books]\n",
    "\n",
    "    # Build the dataset    \n",
    "    text = ''\n",
    "    for t in text_list:\n",
    "        text += t\n",
    "\n",
    "    return text\n",
    "\n",
    "def Clean(input_text):\n",
    "    # Lower case\n",
    "    text = input_text.lower()\n",
    "\n",
    "    # Remove space after a new line\n",
    "    text = re.sub('\\n[ ]+\\n', '\\n', text)\n",
    "\n",
    "    # Substitute cases\n",
    "    text = re.sub('['+'_'+']', ' ', text)\n",
    "    text = re.sub('['+'—'+']', ' ', text)\n",
    "    text = re.sub('['+'-'+']', ' ', text)\n",
    "    text = re.sub('['+'“'+']', ' ', text)\n",
    "    text = re.sub('['+'”'+']', ' ', text) \n",
    "    text = re.sub('['+'‘'+']', ' ', text) \n",
    "    text = re.sub('['+'’'+']', ' ', text) \n",
    "    # Keep just points and commas\n",
    "    text = re.sub('['+','+']', ' '+'commapunct'+' ', text)\n",
    "    text = re.sub('['+'.'+']', ' '+'pointpunct'+' ', text)\n",
    "    text = re.sub('['+'!'+']', ' '+'exclapunct'+' ', text)\n",
    "    text = re.sub('['+'?'+']', ' '+'questpunct'+' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ubc-H7IW4dH7"
   },
   "outputs": [],
   "source": [
    "class BookDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, books, boolW2V, w2v_size, crop_len, transform=None):\n",
    "\n",
    "        ### Preprocessing\n",
    "\n",
    "        text = BuildText(path, books)      \n",
    "        text = Clean(text)   \n",
    "        self.text=text  \n",
    "             \n",
    "        ### Paragraph\n",
    "        \n",
    "        # Extract the paragraph\n",
    "        par_list = re.split('\\n\\n', self.text)\n",
    "        \n",
    "\n",
    "        # Split paragraphs\n",
    "        translator=str.maketrans('','',string.punctuation)\n",
    "        par_list = [x.translate(translator).split() for x in par_list]\n",
    "        par_list = [x for x in par_list if len(x) >= crop_len]\n",
    "        \n",
    "        # Considering all* the sequences inside a paragraph\n",
    "        seq_list = []\n",
    "        for par in par_list:\n",
    "            index = len(par)-crop_len\n",
    "            while index > 0:\n",
    "                seq_list.append(crop(par,index,crop_len))\n",
    "                index -= crop_len\n",
    "\n",
    "        random.shuffle(seq_list)\n",
    "\n",
    "        ### Store data\n",
    "        self.par_list = par_list\n",
    "        self.seq_list = seq_list  \n",
    "        self.w2v_size = w2v_size\n",
    "        self.crop_len = crop_len  \n",
    "        self.transform = transform    \n",
    "\n",
    "        # Indexes\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        # Approximations\n",
    "        self.w2a = {}\n",
    "\n",
    "        self.W2V(boolW2V)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_list)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Get paragraph text\n",
    "        text = self.seq_list[idx]\n",
    "        # Encode with numbers\n",
    "        encoded = encode_text(self.w2i,text)\n",
    "        # Create sample\n",
    "        sample = {'text': text, 'encoded': encoded}\n",
    "\n",
    "        # Transform\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def W2V(self, bool):\n",
    "\n",
    "        if bool:\n",
    "\n",
    "            ### Word to vector\n",
    "\n",
    "            # Model\n",
    "            word_model = gensim.models.Word2Vec(self.seq_list, size=self.w2v_size, min_count=1, window=5, workers=4, iter=200)\n",
    "            words = list(word_model.wv.vocab)\n",
    "\n",
    "            # Dictionaries for word2vec     \n",
    "            # From index to word\n",
    "            self.w2i = {w: i for i, w in enumerate(word_model.wv.index2word)}           \n",
    "            # From word to index        \n",
    "            self.i2w = {word_model.wv.vocab[w].index : w for w in words}   \n",
    "            # From words to approximation\n",
    "            self.w2a = { w : ' '+w for w in words }\n",
    "            self.w2a['commapunct'] = ','\n",
    "            self.w2a['pointpunct'] = '.'\n",
    "            self.w2a['exclapunct'] = '!'\n",
    "            self.w2a['questpunct'] = '?'\n",
    "\n",
    "            ### Save\n",
    "\n",
    "            # Save weights\n",
    "            weights = torch.FloatTensor(word_model.wv.vectors)\n",
    "            save_weights = path+'Embedding.torch'\n",
    "            pathw = F\"{save_weights}\" \n",
    "            torch.save(weights, pathw)\n",
    "            # Save dictionaries\n",
    "            with open(path + 'i2w.json', 'w') as f:\n",
    "                json.dump(self.i2w, f, indent=4)\n",
    "            with open(path + 'w2i.json', 'w') as f:\n",
    "                json.dump(self.w2i, f, indent=4)\n",
    "            with open(path + 'w2a.json', 'w') as f:\n",
    "                json.dump(self.w2a, f, indent=4)\n",
    "\n",
    "\n",
    "            # Save model\n",
    "            word_model.save(path+'word_model.bin')\n",
    "\n",
    "            \n",
    "            # Load dictionaries\n",
    "            self.i2w = json.load(open(path + 'i2w.json'))\n",
    "            self.w2i = json.load(open(path + 'w2i.json'))\n",
    "            self.w2a = json.load(open(path + 'w2a.json'))\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Load dictionaries\n",
    "            self.i2w = json.load(open(path + 'i2w.json'))\n",
    "            self.w2i = json.load(open(path + 'w2i.json'))\n",
    "            self.w2a = json.load(open(path + 'w2a.json'))\n",
    "\n",
    "                \n",
    "    def Info(self):    \n",
    "\n",
    "        print('Information about dataset')\n",
    "        print('-------------------------')\n",
    "        print('Number of paragraph:',len(self.par_list))\n",
    "        print('Number of sequences:',len(self.seq_list))\n",
    "        print('Number of vocabs:', len(self.i2w))\n",
    "        print('-------------------------')\n",
    "\n",
    "    def LenVocab(self):\n",
    "        return len(self.i2w)\n",
    "\n",
    "\n",
    "### Encode and Decode functions\n",
    "        \n",
    "def encode_text(dictionary,text):\n",
    "    i = -1\n",
    "    for w in text:\n",
    "        i+=1\n",
    "        try:\n",
    "            temp = dictionary[w]\n",
    "        except:\n",
    "            text.remove(w)\n",
    "    encoded = [dictionary[w] for w in text]\n",
    "    return encoded\n",
    "def decode_text(dictionary,code):\n",
    "    text = [dictionary[str(i)] for i in code]\n",
    "    return text\n",
    "\n",
    "### Crop to build sentences \n",
    "\n",
    "def crop(par,index,crop_len):   \n",
    "    start_idx = index\n",
    "    end_idx = start_idx + crop_len\n",
    "    return par[start_idx: end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qG79r13FrSd5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Class to prepare text for training\n",
    "               \n",
    "class ToTensor():    \n",
    "    def __call__(self, sample):\n",
    "        # Convert one hot encoded text to pytorch tensor\n",
    "        encoded = torch.LongTensor(sample['encoded'])\n",
    "        return {'encoded': encoded} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2107,
     "status": "ok",
     "timestamp": 1581882955898,
     "user": {
      "displayName": "Marino Braghetto",
      "photoUrl": "",
      "userId": "02551736165510158038"
     },
     "user_tz": -60
    },
    "id": "aniCTiLy88Wl",
    "outputId": "0a54504f-d640-454f-f4b6-5207a22029e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about dataset\n",
      "-------------------------\n",
      "Number of paragraph: 3549\n",
      "Number of sequences: 21665\n",
      "Number of vocabs: 17246\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Transformation\n",
    "trans = ToTensor()\n",
    "# Dataset\n",
    "dataset = BookDataset(path,books,boolW2V,w2v_size,crop_len,transform=trans)\n",
    "dataset.Info()\n",
    "vocab_len = dataset.LenVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vq3wk5aXYy56"
   },
   "outputs": [],
   "source": [
    "# Check on the length of the sentences\n",
    "for seq in dataset:\n",
    "  if len(seq['encoded'])!=20:\n",
    "    print(len(seq['encoded']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koeL-foiWWLx"
   },
   "source": [
    "# Analysis of Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2961,
     "status": "ok",
     "timestamp": 1581882956768,
     "user": {
      "displayName": "Marino Braghetto",
      "photoUrl": "",
      "userId": "02551736165510158038"
     },
     "user_tz": -60
    },
    "id": "ssOdsV5RWVYu",
    "outputId": "da322e97-e0f5-4f18-d441-bd17f09451d6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEGCAYAAACO8lkDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(path+'word_model.bin')\n",
    "\n",
    "name = ['stairs','window','table','morning','night','afternoon','woman','man','child']\n",
    "vector = model[name]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(vector)\n",
    "\n",
    "plt.scatter(result[:,0],result[:,1])\n",
    "for i, word in enumerate(name):\n",
    "    plt.annotate(word,xy=(result[i,0],result[i,1]),fontsize=11)\n",
    "plt.xlim(-11,+16)\n",
    "plt.ylim(-11,+16)\n",
    "plt.xticks(np.arange(-10,16,5))\n",
    "plt.yticks(np.arange(-10,16,5))\n",
    "plt.xlabel('First component')\n",
    "plt.ylabel('Second component')\n",
    "plt.savefig(path+'w2v.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V4VSiRzm2-4Z"
   },
   "source": [
    "# Training and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6116,
     "status": "ok",
     "timestamp": 1581882959937,
     "user": {
      "displayName": "Marino Braghetto",
      "photoUrl": "",
      "userId": "02551736165510158038"
     },
     "user_tz": -60
    },
    "id": "l5F69TzXpE2b",
    "outputId": "42e09221-bbbe-4db4-b1bb-2ebb4d56cd5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about training and validation set\n",
      "---------------------------------------------\n",
      "Size of training set: 17332\n",
      "Size of validation set: 4333\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(int(17))\n",
    "train, val = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8),len(dataset)-int(len(dataset)*0.8)])\n",
    "print('Information about training and validation set')\n",
    "print('---------------------------------------------')\n",
    "print('Size of training set:',len(train))\n",
    "print('Size of validation set:',len(val))\n",
    "print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3piu4BNF7TQK"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0bWCR5aOqBme"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_len, w2v_size, hidden_units, layers_num, dropout_prob=0):\n",
    "        # Call the parent init function (required!)\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input layer for embedding\n",
    "        #read weights\n",
    "        w2vweight = torch.load(path+'Embedding.torch')\n",
    "        w2vweight = torch.FloatTensor(w2vweight)\n",
    "\n",
    "        #vocab_len -> w2v_size\n",
    "        self.embedding = nn.Embedding(vocab_len, w2v_size).from_pretrained(w2vweight)\n",
    "        # Do not train the embedding layer\n",
    "        self.embedding.weight.requires_grad=False  \n",
    "\n",
    "        # LSTM \n",
    "        #w2v_size -> hidden_units\n",
    "        self.rnn = nn.LSTM(input_size=w2v_size, \n",
    "                           hidden_size=hidden_units,\n",
    "                           num_layers=layers_num,\n",
    "                           dropout=dropout_prob,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # Output\n",
    "        #hidden_units -> vocab_len\n",
    "        self.out = nn.Linear(hidden_units, vocab_len)\n",
    "        \n",
    "    def forward(self, x, state=None):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)\n",
    "        # LSTM\n",
    "        x, rnn_state = self.rnn(x, state)\n",
    "        # Linear layer\n",
    "        output = self.out(x)\n",
    "\n",
    "        return output, rnn_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIsoEXMdPWdP"
   },
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbVGqzqGPVmp"
   },
   "outputs": [],
   "source": [
    " class TrainPredict():\n",
    "\n",
    "    def __init__(self, net, dataset, train, val, batch_size):\n",
    "        self.net = net\n",
    "        self.dataset = dataset\n",
    "        # Saving bacthes\n",
    "        self.train = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "        self.val = DataLoader(val, batch_size=batch_size, shuffle=True, num_workers=1)      \n",
    "        # Saving loss\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "\n",
    "        # Print information about batches\n",
    "        print('-------------------------------------------')\n",
    "        print('Number of training batches:', ceil(len(train)/batch_size))\n",
    "        print('Number of validation batches:', ceil(len(val)/batch_size))\n",
    "        print('-------------------------------------------')\n",
    "        \n",
    "    # Train a single batch\n",
    "    def train_batch(self, e, batch, loss_fn, optimizer):\n",
    "\n",
    "        # Get the labels (the last letter of each sequence) and input\n",
    "        X, y = batch[:, :-1], batch[:, 1:]\n",
    "        # Eventually clear previous recorded gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        y_pred, _ = self.net(X)\n",
    "        # Evaluate loss only for last output\n",
    "        loss = loss_fn(y_pred.transpose(1, 2), y)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update\n",
    "        optimizer.step()\n",
    "     \n",
    "        # Return average batch loss\n",
    "        return float(loss.data)\n",
    "\n",
    "    # Evaluate a single batch\n",
    "    def eval_batch(self, e, batch, loss_fn):\n",
    " \n",
    "        # Get the labels (the last letter of each sequence) and input\n",
    "        X, y = batch[:, :-1], batch[:, 1:]\n",
    "        with torch.no_grad(): # Avoid tracking the gradients (much faster!)\n",
    "            y_pred, _ = self.net(X)\n",
    "        # Evaluate loss only for last output\n",
    "        loss = loss_fn(y_pred.transpose(1, 2), y)\n",
    "\n",
    "        # Return average batch loss\n",
    "        return float(loss.data)\n",
    "\n",
    "    # Training function\n",
    "    def training(self, n_epochs, verbose):\n",
    "        \n",
    "        if useCuda:\n",
    "          torch.cuda.empty_cache()\n",
    "\n",
    "        # Define Optimizer, Scheduler and Loss \n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.net.parameters()),lr=1e-3, weight_decay=5e-4)\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Epochs\n",
    "        lt = []\n",
    "        lv = []\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "                if((epoch+1)%2 == 0 and verbose):\n",
    "                      print('##################################')\n",
    "                      print('## EPOCH %d' % (epoch + 1))\n",
    "                      print('##################################')\n",
    "\n",
    "                ### Training\n",
    "\n",
    "                # Iterate batches\n",
    "                # Training mode\n",
    "                self.net.train() \n",
    "                for batch_sample in self.train:\n",
    "                    # Extract batch\n",
    "                    batch = batch_sample['encoded'].to(device)\n",
    "                    # Update network\n",
    "                    batch_loss = self.train_batch(epoch, batch, loss_fn, optimizer)\n",
    "                    # Save loss\n",
    "                    lt.append(batch_loss)            \n",
    "\n",
    "                if((epoch+1)%2 == 0 and verbose):\n",
    "                    print('\\t Training loss:', np.mean(np.array(lt)))\n",
    "\n",
    "\n",
    "                if((epoch+1)%1 == 0):\n",
    "                    self.loss_train.append(np.mean(np.array(lt))) \n",
    "                    lt = []\n",
    "\n",
    "                ### Validation\n",
    "\n",
    "                # Iterate batches\n",
    "                # Eval mode\n",
    "                self.net.eval()\n",
    "                for batch_sample in self.val:\n",
    "                    # Extract batch\n",
    "                    batch = batch_sample['encoded'].to(device)\n",
    "                    # Update network\n",
    "                    batch_loss = self.eval_batch(epoch, batch, loss_fn)\n",
    "                    # Save loss\n",
    "                    lv.append(batch_loss)\n",
    "\n",
    "                if((epoch+1)%2 == 0  and verbose):\n",
    "                      print('\\t Validation loss :', np.mean(np.array(lv)))\n",
    "\n",
    "                if((epoch+1)%1 == 0):\n",
    "                    self.loss_val.append(np.mean(np.array(lv)))\n",
    "                    lv = []                     \n",
    "\n",
    "                # Save weights\n",
    "                if (epoch+1)%5:\n",
    "                    # Save network parameters\n",
    "                    torch.save(self.net.state_dict(), path+'net_params.pth')\n",
    "\n",
    "                if epoch > 30:\n",
    "                    # prevent overfitting\n",
    "                    k = 0\n",
    "                    for i in range(30):\n",
    "                      index=len(self.loss_val)-1\n",
    "                      if self.loss_val[index-i]>self.loss_val[index-i-1]: \n",
    "                        k+=1\n",
    "                    if k>= 20: \n",
    "                        print('Early stopping at epoch', epoch)\n",
    "                        ### Save final weights                \n",
    "                        # Save network parameters\n",
    "                        torch.save(self.net.state_dict(),path+'net_params.pth')\n",
    "                        break\n",
    "\n",
    "        ### Save final weights                \n",
    "        torch.save(self.net.state_dict(), path+ 'net_params.pth')\n",
    "\n",
    "        return self.loss_train, self.loss_val\n",
    "\n",
    "    # Prediction\n",
    "    def predict(self,input_seed,n):\n",
    "\n",
    "        seed = Clean(input_seed)\n",
    "        translator=str.maketrans('','',string.punctuation)\n",
    "        seed = seed.translate(translator).split() \n",
    "\n",
    "        # Evaluation mode\n",
    "        self.net.eval() \n",
    "\n",
    "        ###  Find initial state of the RNN\n",
    "        with torch.no_grad():\n",
    "            # Encode seed\n",
    "            seed_encoded = encode_text(self.dataset.w2i, seed)\n",
    "            # To tensor\n",
    "            seed_tensor = torch.LongTensor(seed_encoded)\n",
    "            # Add batch axis\n",
    "            seed_tensor = seed_tensor.unsqueeze(0).to(device)\n",
    "            # Forward pass\n",
    "            net_out, net_state = self.net(seed_tensor)\n",
    "        # Get the most probable last output index\n",
    "        next_encoded = self.next_softmax(net_out[:, -1, :])\n",
    "        # Print the seed letters\n",
    "        print(input_seed, end='', flush=True)\n",
    "\n",
    "        next_decoded = self.dataset.i2w[str(next_encoded)]\n",
    "        print(self.dataset.w2a[next_decoded], end='', flush=True)\n",
    "\n",
    "        ### Generate sentences\n",
    "\n",
    "        tot_count = 0\n",
    "        while True:\n",
    "            with torch.no_grad(): # No need to track the gradients\n",
    "                # The new network input is the one hot encoding of the last chosen letter\n",
    "                net_input = torch.LongTensor([next_encoded])\n",
    "                net_input = net_input.unsqueeze(0).to(device)\n",
    "                # Forward pass\n",
    "                net_out, net_state = self.net(net_input, net_state)\n",
    "                # Get the most probable letter index\n",
    "                next_encoded = self.next_softmax(net_out[:, -1, :])\n",
    "               \n",
    "                # Decode the letter\n",
    "                next_decoded = self.dataset.i2w[str(next_encoded)]\n",
    "                print(self.dataset.w2a[next_decoded], end='', flush=True)\n",
    "                # Count total letters\n",
    "                tot_count += 1\n",
    "                # Break if n words\n",
    "                if tot_count > n:\n",
    "                    break\n",
    "\n",
    "    def next_softmax(self,x):\n",
    "        #returns the word\n",
    "        with torch.no_grad():\n",
    "            out = nn.functional.softmax(x,dim=1)\n",
    "            vocab = np.arange(out.shape[1])\n",
    "            sampling = out.reshape(-1,).cpu().detach().numpy()\n",
    "            predicted = np.random.choice(vocab, p=sampling/sum(sampling))\n",
    "        return predicted.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhcO2--TG3IE"
   },
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfxXLZZYWCwn"
   },
   "outputs": [],
   "source": [
    "class CrossValidation():\n",
    "    def __init__(self, dataset, train, folds, layers, hidden_units):\n",
    "        self.dataset = dataset\n",
    "        self.train = train\n",
    "        self.folds = folds\n",
    "        self.layers = layers\n",
    "        self.hidden_units = hidden_units\n",
    "        self.losses = []\n",
    "        self.best = []\n",
    "    \n",
    "    def one_par(self, par, batch_size):\n",
    "\n",
    "        # Print\n",
    "        print('---------------------------------------------------------------')\n",
    "        print('Number layers:',self.layers[par])\n",
    "        print('Hidden units:',self.hidden_units[par])\n",
    "        print('---------------------------------------------------------------')\n",
    "\n",
    "        # Losses\n",
    "        l = 0\n",
    "        # Kfold\n",
    "        kf = KFold(n_splits=self.folds,random_state=17)\n",
    "        kf.get_n_splits(self.train)\n",
    "\n",
    "        # Start cross validation\n",
    "        for train_index, val_index in kf.split(self.train):\n",
    "            train_index = list(train_index)\n",
    "            val_index = list(val_index)\n",
    "            ktrain = torch.utils.data.Subset(self.train, train_index)\n",
    "            kval= torch.utils.data.Subset(self.train, val_index)\n",
    "            # Build network\n",
    "            knet = Network(vocab_len, w2v_size, self.hidden_units[par], self.layers[par], dropout_prob)\n",
    "            knet = knet.to(device) \n",
    "            print(knet)\n",
    "            TP = TrainPredict(knet, self.dataset, ktrain, kval, batch_size)\n",
    "            print('Prediction before training:')\n",
    "            seed = 'You are'\n",
    "            TP.predict(seed,20)\n",
    "\n",
    "            # Training            \n",
    "            losst, lossv =  TP.training(100, False)\n",
    "\n",
    "            # Plot\n",
    "            e = np.arange(len(losst))\n",
    "            plt.plot(e,losst, label='Training loss')\n",
    "            plt.plot(e,lossv, label='Validation loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            seed = 'The people want'\n",
    "            TP.predict(seed,30)\n",
    "            print('\\n')\n",
    "            seed = 'There were'\n",
    "            TP.predict(seed,30)\n",
    "            print('\\n')\n",
    "            seed = 'She was going to'\n",
    "            TP.predict(seed,30)\n",
    "            print('\\n','---------------------------------------------------------------')\n",
    "\n",
    "            l += lossv[-1]\n",
    "\n",
    "        # Save losses\n",
    "        self.losses.append(l/self.folds)\n",
    "        \n",
    "        print('Loss:',self.losses[-1])\n",
    "\n",
    "    def Validate(self, bool, batch_size):\n",
    "\n",
    "        if bool:\n",
    "          for par in range(len(self.layers)):\n",
    "              self.one_par(par, batch_size)\n",
    "\n",
    "          best = np.argmin(np.array(self.losses))\n",
    "          self.best.append(self.hidden_units[best])\n",
    "          self.best.append(self.layers[best])\n",
    "          self.best.append(self.losses[best])\n",
    "          return self.best\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ye74jS2MQmtr"
   },
   "source": [
    "# Result of the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "BQwkoEFT8mfR"
   },
   "outputs": [],
   "source": [
    "cv = CrossValidation(dataset,train,3,layers=[2,2,2],hidden_units=[32,64,128])\n",
    "best = cv.Validate(boolCV, batch_size)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N2JdmIJFjgVB"
   },
   "outputs": [],
   "source": [
    "cv = CrossValidation(dataset,train,3,layers=[3,3,3],hidden_units=[32,64,128])\n",
    "best = cv.Validate(boolCV, batch_size)\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VLXlnKHvHHgl"
   },
   "source": [
    "# Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2vMJk-0Gt4H"
   },
   "outputs": [],
   "source": [
    "if boolFT:\n",
    "  best = [128,2]\n",
    "  net = Network(vocab_len, w2v_size, best[0], best[1], dropout_prob)\n",
    "  net = net.to(device) \n",
    "  print(net)\n",
    "  TP = TrainPredict(net, dataset, train, val, batch_size)\n",
    "  losst, lossv =  TP.training(500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cglck2X7Gud8"
   },
   "outputs": [],
   "source": [
    "if boolFT:\n",
    "  # Plot\n",
    "  plt.plot(losst, label='Training loss')\n",
    "  plt.plot(lossv, label='Validation loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  #plt.yscale('log')\n",
    "  plt.savefig(path+'loss.png')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WcS57Gn9ZEZ"
   },
   "outputs": [],
   "source": [
    "if boolFT:\n",
    "  seed = 'The people want'\n",
    "  TP.predict(seed,30)\n",
    "  print('\\n')\n",
    "  seed = 'There were'\n",
    "  TP.predict(seed,30)\n",
    "  print('\\n')\n",
    "  seed = 'She was going to'\n",
    "  TP.predict(seed,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ivBey6wRluY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "HW3-Braghetto-CODE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
